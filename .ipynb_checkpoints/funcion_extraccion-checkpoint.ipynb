{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e97b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_informacion(url, archivo_csv):\n",
    "    #Importamos librerias\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.parse\n",
    "    import csv\n",
    "\n",
    "    # Crea una lista para almacenar los enlaces\n",
    "    link_list = []\n",
    "\n",
    "    # Establece un límite para el número de páginas a procesar\n",
    "    limit = 500\n",
    "\n",
    "    # Establece el offset inicial\n",
    "    of = 0\n",
    "\n",
    "    while of<limit:\n",
    "        # Realiza una solicitud HTTP a la página web con el offset actual\n",
    "        response = requests.get(f\"{url}/recent-submissions?offset={of}\")\n",
    "\n",
    "        # Obtiene el HTML de la respuesta\n",
    "        html = response.text\n",
    "\n",
    "        # Crea un objeto BeautifulSoup a partir del HTML\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Parsea la URL\n",
    "        parsed_url = urllib.parse.urlparse(\"{}/recent-submissions?offset={}\".format(url, of))\n",
    "        # Obtiene el contenido principal de la URL (todo antes del /repositorio)\n",
    "        url_base = parsed_url.scheme + '://' + parsed_url.netloc\n",
    "\n",
    "        # Busca todos los enlaces <a> en el HTML con clase image-link \n",
    "        # Esto permite buscar los hrefs que estan ligados a las imagenes(thumbnails) de cada tesis\n",
    "        # Solo funciona si el repositorio usa thumbnails\n",
    "        links = soup.find_all(\"a\",{\"class\":\"image-link\"})\n",
    "        hrefs1 = [url_base + a[\"href\"] for a in links]\n",
    "\n",
    "        # Itera sobre cada enlace y agrega su atributo \"href\" a la lista\n",
    "        for link in hrefs1:\n",
    "            link_list.append(link)\n",
    "\n",
    "        # Incrementa el offset para la siguiente iteración\n",
    "        of += 20\n",
    "\n",
    "    # Acesso a la tabla de metadatos añadiendo ?show=full\n",
    "    links_full = [link + '?show=full' for link in link_list]\n",
    "\n",
    "    # Abre un archivo CSV en modo escritura\n",
    "    with open(archivo_csv, 'w', newline='') as csvfile:\n",
    "        # Crea el objeto escritor\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Escribe la cabecera del archivo CSV\n",
    "        writer.writerow(['Autores', 'Asesores', 'Título', 'Resumen', 'Año', 'Institución', 'Grado'])\n",
    "\n",
    "        # Recorre cada link\n",
    "        for link in links_full:\n",
    "            # Descarga el contenido del link\n",
    "            html = requests.get(link).text\n",
    "            # Parsea el contenido del link\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # Obtiene la información de cada link\n",
    "            \n",
    "            # Autores: como pueden ser varios se usa un bucle for\n",
    "            authors = [element['content'] for element in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "            \n",
    "            # Asesores y Grado: se usa find_next_sibling porque existe un problema de identificación con la etiqueta \"td\"\n",
    "            asesores = [element.find_next_sibling().text for element in soup.find_all('td', string='dc.contributor.advisor')]\n",
    "            grado = [element.find_next_sibling().text for element in soup.find_all('td', string='thesis.degree.name')]\n",
    "            \n",
    "            # Titulo, Resumen, Año e Institución: cuando la etiqueta es \"meta\" es más fácil de encontrar\n",
    "            titulo = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "            abstract = soup.find('meta', {'name': \"DCTERMS.abstract\"})['content']\n",
    "            año = soup.find('meta', {'name': \"citation_date\"})['content']\n",
    "            institucion = soup.find('meta', {'name': 'citation_publisher'})['content']\n",
    "            \n",
    "\n",
    "            # Escribe la fila en el archivo CSV\n",
    "            writer.writerow([authors, asesores, titulo, abstract, año, institucion, grado])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fae2323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraemos 3 csv de los 3 repositorios\n",
    "extraer_informacion(\"https://repositorio.esan.edu.pe/handle/20.500.12640/1814\", \"esan_repo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraer_informacion(\"https://tesis.pucp.edu.pe/repositorio/handle/20.500.12404/757\", \"pucp_repo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af62bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraer_informacion(\"https://repositorioacademico.upc.edu.pe/handle/10757/621395\", \"upc_repo.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
